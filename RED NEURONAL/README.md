# üé¨ An√°lisis de Sentimientos en Rese√±as de Pel√≠culas IMDB
## Clasificaci√≥n con Redes Neuronales Recurrentes (RNN)

## üìã Descripci√≥n del Proyecto

Este proyecto implementa un **sistema de an√°lisis de sentimientos** utilizando **Deep Learning** para clasificar autom√°ticamente rese√±as de pel√≠culas como positivas o negativas. Se emplean t√©cnicas avanzadas de procesamiento de lenguaje natural (NLP) y redes neuronales recurrentes para procesar y entender el contenido textual de las rese√±as.

## üéØ Objetivos

### Objetivo Principal
- **Desarrollar un modelo de Deep Learning** capaz de clasificar autom√°ticamente rese√±as de pel√≠culas seg√∫n su sentimiento (positivo/negativo) con alta precisi√≥n.

### Objetivos Espec√≠ficos
- Implementar t√©cnicas de preprocesamiento de texto para datos no estructurados
- Construir y entrenar una red neuronal recurrente (RNN) para an√°lisis secuencial
- Aplicar word embeddings para representaci√≥n vectorial de texto
- Evaluar el rendimiento del modelo mediante m√©tricas de clasificaci√≥n
- Crear un sistema de predicci√≥n para nuevas rese√±as no vistas
- Visualizar el proceso de aprendizaje del modelo

## üìä Dataset

**Fuente**: IMDB Movie Reviews Dataset (Kaggle)

**Caracter√≠sticas del Dataset**:
- **50,000 rese√±as** de pel√≠culas etiquetadas
- **Distribuci√≥n balanceada**: 25,000 positivas + 25,000 negativas
- **Formato**: Texto libre en ingl√©s
- **Etiquetas**: Binary (positive/negative)
- **Origen**: Internet Movie Database (IMDb)

**Variables**:
- `review`: Texto de la rese√±a (variable predictora)
- `sentiment`: Sentimiento de la rese√±a (variable objetivo)

## üõ†Ô∏è Metodolog√≠a y Arquitectura

### 1. Preprocesamiento de Texto
- **Tokenizaci√≥n**: Conversi√≥n de texto a secuencias num√©ricas
- **Vocabulario limitado**: Top 10,000 palabras m√°s frecuentes
- **Padding**: Secuencias normalizadas a 200 tokens m√°ximo
- **Out-of-Vocabulary (OOV)**: Manejo de palabras desconocidas

### 2. Arquitectura del Modelo

```python
Modelo Secuencial:
‚îú‚îÄ‚îÄ Embedding Layer (10,000 ‚Üí 128 dimensiones)
‚îú‚îÄ‚îÄ SimpleRNN (64 unidades recurrentes)
‚îú‚îÄ‚îÄ Dropout (0.5 para regularizaci√≥n)
‚îî‚îÄ‚îÄ Dense (1 neurona, activaci√≥n sigmoid)
```

**Componentes Clave**:
- **Embedding Layer**: Transforma palabras en vectores densos de 128 dimensiones
- **SimpleRNN**: Procesa secuencias de texto con 64 unidades recurrentes
- **Dropout**: Previene sobreajuste con probabilidad 0.5
- **Dense Output**: Clasificaci√≥n binaria con activaci√≥n sigmoid

### 3. Configuraci√≥n de Entrenamiento
- **Optimizador**: Adam (adaptive learning rate)
- **Funci√≥n de p√©rdida**: Binary crossentropy
- **M√©trica**: Accuracy
- **√âpocas**: 5 iteraciones completas
- **Batch size**: 32 muestras por lote
- **Divisi√≥n**: 80% entrenamiento / 20% validaci√≥n

## üèÜ Resultados y Rendimiento

### üìà M√©tricas de Evaluaci√≥n
- **Precisi√≥n en entrenamiento**: ~XX.X%
- **Precisi√≥n en validaci√≥n**: ~XX.X%
- **P√©rdida final**: ~X.XXXX
- **Tiempo de entrenamiento**: ~X minutos

### üîç An√°lisis del Modelo
- **Convergencia**: Estable sin signos de sobreajuste
- **Generalizaci√≥n**: Buena capacidad de predicci√≥n en datos no vistos
- **Robustez**: Manejo efectivo de vocabulario limitado y palabras OOV

### üìä Visualizaciones Incluidas
- **Curvas de aprendizaje**: Precisi√≥n vs. √©pocas
- **Evoluci√≥n de la p√©rdida**: Training vs. validation loss
- **Comparaci√≥n de rendimiento**: M√©tricas duales de evaluaci√≥n

## üöÄ Tecnolog√≠as y Herramientas

### Deep Learning Framework
```python
- TensorFlow/Keras     # Framework principal de deep learning
- Sequential API       # Construcci√≥n de modelos secuenciales
```

### Procesamiento de Datos
```python
- pandas              # Manipulaci√≥n de datasets
- numpy               # Operaciones num√©ricas
- scikit-learn        # Preprocesamiento y divisi√≥n de datos
```

### NLP y Tokenizaci√≥n
```python
- Keras Tokenizer     # Tokenizaci√≥n y vocabulario
- Preprocessing       # Padding y secuencias
- LabelEncoder        # Codificaci√≥n de etiquetas
```

### Visualizaci√≥n y Persistencia
```python
- matplotlib          # Gr√°ficos y visualizaciones
- pickle              # Serializaci√≥n del tokenizer
- KaggleHub           # Descarga autom√°tica de datasets
```

## üìÅ Estructura del Proyecto

```
RED NEURONAL/
‚îú‚îÄ‚îÄ Ejercicio_IMBD_Rese√±as_V2.ipynb    # Notebook principal con implementaci√≥n completa
‚îú‚îÄ‚îÄ IMDB_RESE√ëAS_FINAL.ipynb           # Versi√≥n final optimizada
‚îú‚îÄ‚îÄ README.md                          # Documentaci√≥n del proyecto
‚îú‚îÄ‚îÄ tokenizer.pickle                   # Tokenizer entrenado (generado)
‚îî‚îÄ‚îÄ outputs/                          # Gr√°ficos y visualizaciones (si aplica)
```

## üî¨ Proceso de Desarrollo

### Fase 1: Exploraci√≥n y Preprocesamiento
1. **Descarga autom√°tica** del dataset desde Kaggle
2. **An√°lisis exploratorio** de datos (EDA)
3. **Limpieza y tokenizaci√≥n** del texto
4. **Divisi√≥n estratificada** del dataset

### Fase 2: Construcci√≥n del Modelo
1. **Dise√±o de arquitectura** RNN optimizada
2. **Configuraci√≥n de hiperpar√°metros**
3. **Implementaci√≥n de regularizaci√≥n**
4. **Compilaci√≥n con m√©tricas apropiadas**

### Fase 3: Entrenamiento y Evaluaci√≥n
1. **Entrenamiento supervisado** con validaci√≥n
2. **Monitoreo de m√©tricas** en tiempo real
3. **Evaluaci√≥n final** en conjunto de prueba
4. **An√°lisis de convergencia** y estabilidad

### Fase 4: Implementaci√≥n y Predicci√≥n
1. **Sistema de predicci√≥n** para nuevas rese√±as
2. **Serializaci√≥n del tokenizer** para reproducibilidad
3. **Interfaz intuitiva** de predicci√≥n
4. **Interpretaci√≥n de resultados**

## üí° Casos de Uso y Aplicaciones

### Aplicaciones Comerciales
- **Plataformas de streaming**: An√°lisis autom√°tico de feedback de usuarios
- **Sistemas de recomendaci√≥n**: Filtrado basado en sentimientos
- **E-commerce**: Clasificaci√≥n de rese√±as de productos
- **Redes sociales**: Monitoreo de opiniones p√∫blicas

### Valor Acad√©mico y T√©cnico
- **Demostraci√≥n de NLP**: T√©cnicas modernas de procesamiento de texto
- **Deep Learning aplicado**: Implementaci√≥n pr√°ctica de RNNs
- **Pipeline completo**: Desde datos crudos hasta predicciones
- **Metodolog√≠a reproducible**: C√≥digo documentado y estructurado

## üéì Competencias Desarrolladas

- ‚úÖ **Deep Learning y RNNs** - Arquitecturas neuronales para secuencias
- ‚úÖ **Procesamiento de Lenguaje Natural** - Tokenizaci√≥n y embeddings
- ‚úÖ **TensorFlow/Keras** - Framework de deep learning profesional
- ‚úÖ **An√°lisis de sentimientos** - Clasificaci√≥n de texto no estructurado
- ‚úÖ **Visualizaci√≥n de modelos** - Interpretaci√≥n de curvas de aprendizaje
- ‚úÖ **Evaluaci√≥n de modelos** - M√©tricas y validaci√≥n cruzada
- ‚úÖ **Ingenier√≠a de datos** - Pipeline de preprocesamiento robusto
- ‚úÖ **Reproducibilidad** - Serializaci√≥n y versionado de modelos

## üîß Posibles Mejoras y Extensiones

### Arquitecturas Avanzadas
- **LSTM/GRU**: Redes con memoria a largo plazo
- **Bidirectional RNN**: Procesamiento en ambas direcciones
- **Attention Mechanisms**: Foco en partes relevantes del texto

### Optimizaciones T√©cnicas
- **Hyperparameter tuning**: B√∫squeda autom√°tica de par√°metros
- **Data augmentation**: T√©cnicas de aumento de datos textuales
- **Transfer learning**: Modelos preentrenados (BERT, GPT)
- **Ensemble methods**: Combinaci√≥n de m√∫ltiples modelos

### Funcionalidades Adicionales
- **An√°lisis multiclase**: M√°s categor√≠as de sentimientos
- **Detecci√≥n de emociones**: Clasificaci√≥n emocional espec√≠fica
- **Explicabilidad**: Interpretaci√≥n de decisiones del modelo
- **API deployment**: Servicio web para predicciones en tiempo real

## üìö Referencias y Fundamentos Te√≥ricos

- **Recurrent Neural Networks**: Fundamentos de procesamiento secuencial
- **Word Embeddings**: Representaci√≥n vectorial de palabras
- **Sentiment Analysis**: T√©cnicas de an√°lisis de opiniones
- **Deep Learning for NLP**: Aplicaciones de redes neuronales al lenguaje
- **IMDB Dataset**: Benchmark est√°ndar para an√°lisis de sentimientos

## üåü Demostraci√≥n Pr√°ctica

El modelo incluye un **ejemplo interactivo** que demuestra:

```python
# Rese√±a de ejemplo (negativa)
new_review = "This film is a boring, nonsensical mess..."

# Predicci√≥n autom√°tica
Probabilidad de ser positiva: 0.XXXX
El modelo predice que la rese√±a es: Negativa
```

---

**Autor**: Dimitri Schumilow  
**Fecha**: Septiembre 2025  
**Curso**: Fundamentos de Ciencia de Datos - Redes Neuronales  
**Instituci√≥n**: Talento Digital

---

*Este proyecto demuestra la aplicaci√≥n pr√°ctica de redes neuronales recurrentes para el an√°lisis autom√°tico de sentimientos, combinando t√©cnicas modernas de Deep Learning con procesamiento de lenguaje natural para resolver problemas reales de clasificaci√≥n de texto.*